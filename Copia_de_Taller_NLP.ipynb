{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Taller-NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiocastro85/trabajosPython/blob/main/Copia_de_Taller_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMD6waZ2AmPf"
      },
      "source": [
        "# **Taller: introducción al procesamiento del lenguaje natural**\n",
        "\n",
        "Por favor lea con atención **TODAS** las instrucciones para que el taller se le califique de acuerdo a los requerimientos.\n",
        "\n",
        "**Objetivo:** construir un modelo TF-IDF con base en un corpus de documentos  y calcular la similitud coseno entre estos documentos que conforman el corpus.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Primera parte: responder las siguientes preguntas\n",
        "1. Descargue y descomprima el corpus (ejecute la celda indicada)\n",
        "2. ¿Cuántos documentos tiene el corpus?\n",
        "3. Explore dos o tres documentos al azar e indique qué tipo de documentos son los del corpus: cuentos, sinópsis de películas, chistes, novelas, libros, discursos, etc.\n",
        "4. ¿Cuál es el tipo de archivo de los documentos del corpus?\n",
        "\n",
        "---\n",
        "\n",
        "# Segunda parte: flujo de trabajo y responder preguntas planteadas\n",
        "\n",
        "Realice lo siguiente con **cada documento** del corpus:\n",
        "\n",
        "1. Obtener el texto en \"bruto\" (`raw`) de cada documento.\n",
        "2. Convierta todo el texto en bruto en minúsculas. `raw = raw.lower()`.\n",
        "3. Obtener del texto en bruto del numeral anterior (`raw`), una lista de tokens de palabras **usando** `nltk.tokenize.word_tokenize` **pasando como parámetro el lenguaje usado en el contenido de los documentos del corpus**.\n",
        "4. Obtenga una **nueva** lista de tokens de palabras **usando** `nltk.tokenize.RegexTokenizer` y empleando la siguiente expresión regular como parámetro del tokenizador: `'\\w+'`.\n",
        "    1. ¿Qué diferencias encuentra con la lista de tokens del numeral anterior? Sugerencia: imprima ambas listas, mida la longitud y compare.\n",
        "5. Tomando la lista de tokens del punto anterior, crear una **nueva** lista de tokens filtrando (eliminando) las *stop words*.\n",
        "6. Tomando la lista de tokens del punto anterior, crear una **nueva** lista de tokens filtrando (eliminando) de la lista, los tokens que sean de menos de tres caractéres de longitud.\n",
        "7. Tomando la lista de tokens del punto anterior, crear una **nueva** lista de tokens eliminando acentos en las palabras.\n",
        "8. Tomando la lista de tokens del punto anterior, crear una **nueva** lista de tokens obtienendo el *stem* de cada token.\n",
        "9. Agregue como elemento de **una nueva lista**, la lista del punto anterior. Cada lista producida en el numeral ocho debe ser agregada como elemento de esta nueva lista lista. Define esta lista vacia al principio del código y vaya agregando cada una de las listas que va obteniendo en el numeral anterior.\n",
        "10. Crear una **nueva lista** que corresponde al corpus que se le debe entregar a la clase `TfidfVectorizer` de la librería `sklearn`. **Nota: Ver diapositivas de clase**.\n",
        "11. Crear el modelo TF-IDF.\n",
        "    1. ¿Cuántos términos conforman el vocabulario del corpus?\n",
        "12. Medir la similitud entre los documentos usando la medida de similitud coseno.\n",
        "    1. Muestre la matriz de coeficientes de similitud entre los documentos del corpus.\n",
        "    2. ¿Cuáles son los documentos que más se parecen entre sí? ¿Podría verfificar si es cierto? Justifique.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twxefmttKP7N"
      },
      "source": [
        "# **Pseudocódigo de guía para la fase de preprocesamiento del texto**\n",
        "\n",
        "El siguiente pseudocódigo tiene como propósito ilustrar toda la etapa de preprocesamiento de texto indicada en los numerales anteriores. Al finalizar debe obtener el conjunto $ D $ (una lista de Python) que corresponde a una lista de tokens en el formato que se necesita para usar `TfidfVectorizer()` de la librería `Sklearn`. Una vez obtenido $ D $, use el mismo código de Python que encuentra en las diapositivas para construir el modelo TF-IDF.\n",
        "\n",
        "Las listas $ T3, T4, T5, T6, T7 $ y $ T8 $ en el siguiente pseudocódigo corresponden a las listas que se piden en los mismos numerales de la sección anterior.\n",
        "\n",
        "**Convenciones usadas en este pseudocódigo**\n",
        "\n",
        "Símbolo           | Significado\n",
        "------------------|------------------\n",
        "$ \\leftarrow $    | Operador de asignación.\n",
        "$ \\emptyset $     | Conjunto vacío, inicialización de listas, arreglos, etc.\n",
        "$ \\triangleright $| Se usa para indicar comentarios .\n",
        "$ \\cup $          | Operador de unión. P. Ej. agregar elementos a una lista, arreglo.\n",
        "$ \\in $           | Pertenece a.\n",
        "$ \\notin $        | No pertenece a.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "$ imports\\ \\text{...} $\n",
        "\n",
        "$ \\text {Sea}\\ S\\ \\text{un conjunto de stop words} $\n",
        "\n",
        "$ \\text {Sea}\\ A\\ \\text{el conjunto de archivos que conforman el corpus} $\n",
        "\n",
        "$ T  \\leftarrow \\emptyset \\triangleright \\text{Conjunto para construir el corpus de tokens que indica el numeral 9} $\n",
        "\n",
        "$ \\textbf{for all}\\ archivo \\in A\\ \\textbf{do} $\n",
        "\n",
        ">$  a \\leftarrow open(archivo) $\n",
        "\n",
        ">$ raw \\leftarrow a.read() $\n",
        "\n",
        ">$ raw \\leftarrow raw.lower() $\n",
        "\n",
        ">$ a.close() $\n",
        "\n",
        ">$ T3 \\leftarrow word\\_tokenize(raw, language) \\triangleright \\text{Conjunto tokens en bruto} $\n",
        "\n",
        ">$ rt \\leftarrow RegexpTokenizer(\\text{\"\\\\w+\"})$\n",
        "\n",
        ">$ T4 \\leftarrow rt.tokenize(raw) $\n",
        "\n",
        ">$ T5 \\leftarrow \\emptyset \\triangleright \\text{Conjunto tokens sin stop words}$\n",
        "\n",
        ">$ \\textbf{for all}\\ t \\in T4\\ \\textbf{do} $\n",
        "\n",
        ">> $ \\textbf{if}\\ t \\notin S\\ \\textbf{then}$\n",
        "\n",
        ">>> $ T5 \\leftarrow T5 \\cup \\{t\\} $\n",
        "\n",
        ">> $ \\textbf{end if} $\n",
        "\n",
        "> $ \\textbf{end for} $\n",
        "\n",
        ">$ T6 \\leftarrow \\emptyset \\triangleright \\text{Conjunto tokens longitud > 3}$\n",
        "\n",
        ">$ \\textbf{for all}\\ t \\in T5\\ \\textbf{do} $\n",
        "\n",
        ">> $ \\textbf{if}\\ length(t) > 3\\ \\textbf{then}$\n",
        "\n",
        ">>> $ T6 \\leftarrow T6 \\cup \\{t\\} $\n",
        "\n",
        ">> $ \\textbf{end if} $\n",
        "\n",
        "> $ \\textbf{end for} $\n",
        "\n",
        ">$ T7 \\leftarrow \\emptyset \\triangleright \\text{Conjunto tokens normalizado}$\n",
        "\n",
        ">$ \\textbf{for all}\\ t \\in T6\\ \\textbf{do} $\n",
        "\n",
        ">> $ t \\leftarrow unidecode(t) $\n",
        "\n",
        ">>$ T7 \\leftarrow T7 \\cup \\{t\\} $\n",
        "\n",
        "> $ \\textbf{end for} $\n",
        "\n",
        ">$ T8 \\leftarrow \\emptyset \\triangleright \\text{Conjunto tokens stems}$\n",
        "\n",
        ">$ \\textbf{for all}\\ t \\in T7\\ \\textbf{do} $\n",
        "\n",
        ">> $ t \\leftarrow stem(t) $\n",
        "\n",
        ">>$ T8 \\leftarrow T8 \\cup \\{t\\} $\n",
        "\n",
        "> $ \\textbf{end for} $\n",
        "\n",
        "> $ T \\leftarrow T \\cup \\{T8\\} $\n",
        "\n",
        "$ \\textbf{end for} $\n",
        "\n",
        "$ \\triangleright \\text{El siguiente código para el numeral 10} $\n",
        "\n",
        "$ D \\leftarrow \\emptyset \\triangleright \\text{Corpus de tokens para TfidfVectorizer()}$\n",
        "\n",
        "$ \\textbf{for all}\\ e \\in T\\ \\textbf{do}$\n",
        "\n",
        ">$ \\triangleright \\text{Tal cual en Python lo siguiente} $\n",
        "\n",
        ">$ t \\leftarrow \\text{'  '}.join(e) \\triangleright \\text{Va un espacio en blanco en las comillas}$\n",
        "\n",
        ">$ D \\leftarrow D \\cup \\{t\\}$\n",
        "\n",
        "$ \\textbf{end for} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycIVTwBzlkGt"
      },
      "source": [
        "# **Descarga del corpus usado en el taller**\n",
        "\n",
        "Por favor ejecute las siguientes celdas para la descarga y descompresión del corpus. Si tiene algún inconveniente en esta fase por favor haga lo siguiente:\n",
        "\n",
        "1. Clic en el menú **Entorno de ejecución**\n",
        "2. Clic en **Restablecer el estado de fábrica del entorno de ejecución** \n",
        "3. Ejecute de nuevo las celdas de código"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "E294gg3AwTay"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "xWNcvqT6_rqj",
        "outputId": "657eb1cd-da9f-4fa4-d280-058cbb4b3831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 36.1 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "cIDxclk97Lny",
        "outputId": "d7598271-4dd9-4bba-d9d1-e0284263c524",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "MxPnch9I37Ek"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "metadata": {
        "id": "uEOFDoDz5FJf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "5mYHl4MV7akw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unidecode import unidecode"
      ],
      "metadata": {
        "id": "6tgNUuWNA809"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "Y8lEl4DmCT26"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "FuPXw6o84EYa",
        "outputId": "8340b7be-5509-43c1-baf8-4ae85e0c5ce2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Dj4Tt-AhzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f55e0f21-7177-419e-d36f-a42389f221ab"
      },
      "source": [
        "!wget -O CorpusTallerNLP.zip https://drive.google.com/u/0/uc?id=1y_Em0jgijsfxgqrSBdI59Fk_r7P2MeMk&export=download"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-21 00:29:45--  https://drive.google.com/u/0/uc?id=1y_Em0jgijsfxgqrSBdI59Fk_r7P2MeMk\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.8.100, 142.251.8.113, 142.251.8.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.8.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://drive.google.com/uc?id=1y_Em0jgijsfxgqrSBdI59Fk_r7P2MeMk [following]\n",
            "--2022-05-21 00:29:46--  https://drive.google.com/uc?id=1y_Em0jgijsfxgqrSBdI59Fk_r7P2MeMk\n",
            "Reusing existing connection to drive.google.com:443.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0c-30-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ldqj23o8pvhll7jduakc4n5uhj28ajj7/1653092925000/08159721715164229692/*/1y_Em0jgijsfxgqrSBdI59Fk_r7P2MeMk [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-21 00:29:47--  https://doc-0c-30-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ldqj23o8pvhll7jduakc4n5uhj28ajj7/1653092925000/08159721715164229692/*/1y_Em0jgijsfxgqrSBdI59Fk_r7P2MeMk\n",
            "Resolving doc-0c-30-docs.googleusercontent.com (doc-0c-30-docs.googleusercontent.com)... 142.251.8.132, 2404:6800:4008:c15::84\n",
            "Connecting to doc-0c-30-docs.googleusercontent.com (doc-0c-30-docs.googleusercontent.com)|142.251.8.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27402 (27K) [application/zip]\n",
            "Saving to: ‘CorpusTallerNLP.zip’\n",
            "\n",
            "CorpusTallerNLP.zip 100%[===================>]  26.76K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-05-21 00:29:47 (50.9 MB/s) - ‘CorpusTallerNLP.zip’ saved [27402/27402]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtFMN4PykRoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd8d60af-9264-4352-9827-ef36a938b5d9"
      },
      "source": [
        "!unzip -o -d /root CorpusTallerNLP.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  CorpusTallerNLP.zip\n",
            "   creating: /root/CorpusTallerNLP/\n",
            "  inflating: /root/CorpusTallerNLP/01.txt  \n",
            "  inflating: /root/CorpusTallerNLP/02.txt  \n",
            "  inflating: /root/CorpusTallerNLP/03.txt  \n",
            "  inflating: /root/CorpusTallerNLP/04.txt  \n",
            "  inflating: /root/CorpusTallerNLP/05.txt  \n",
            "  inflating: /root/CorpusTallerNLP/06.txt  \n",
            "  inflating: /root/CorpusTallerNLP/07.txt  \n",
            "  inflating: /root/CorpusTallerNLP/08.txt  \n",
            "  inflating: /root/CorpusTallerNLP/09.txt  \n",
            "  inflating: /root/CorpusTallerNLP/10.txt  \n",
            "  inflating: /root/CorpusTallerNLP/11.txt  \n",
            "  inflating: /root/CorpusTallerNLP/12.txt  \n",
            "  inflating: /root/CorpusTallerNLP/13.txt  \n",
            "  inflating: /root/CorpusTallerNLP/14.txt  \n",
            "  inflating: /root/CorpusTallerNLP/15.txt  \n",
            "  inflating: /root/CorpusTallerNLP/16.txt  \n",
            "  inflating: /root/CorpusTallerNLP/17.txt  \n",
            "  inflating: /root/CorpusTallerNLP/18.txt  \n",
            "  inflating: /root/CorpusTallerNLP/19.txt  \n",
            "  inflating: /root/CorpusTallerNLP/20.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Respuesta  uno: 20 documentos"
      ],
      "metadata": {
        "id": "ZXVMWxjdxXEu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoXGOZyxmmcx"
      },
      "source": [
        "# **Establecimiento de la ruta de trabajo**\n",
        "\n",
        "Ejecute la siguiente celda para establecer la ubicación de ejecución del código Python a la misma ubicación de los archivos del corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SHm497qmyQe"
      },
      "source": [
        "corpusdir = '/root/CorpusTallerNLP/'\n",
        "import os\n",
        "os.chdir(corpusdir)\n",
        "archivos_corpus = sorted(os.listdir('.'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8H0Rw4mnMDP"
      },
      "source": [
        "# **Desarrollo del taller**\n",
        "\n",
        "A partir de este punto su código...(buena suerte)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZhyPHyOnLg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc085a83-8c13-41a8-9ed5-2a2a76452390"
      },
      "source": [
        "# Su código aquí o en mas celdas de código\n",
        "for archivo in archivos_corpus:\n",
        "  a=open(archivo)\n",
        "  lineas=a.readlines()\n",
        "  #converti la lista a string\n",
        "  lineas=''.join(lineas)\n",
        "  lineas=lineas.lower()\n",
        "  T3=word_tokenize(lineas,language=\"spanish\")\n",
        "  rt=RegexpTokenizer('\\w+')\n",
        "  T4=rt.tokenize(lineas)\n",
        "  sw=stopwords.words('spanish')\n",
        "  T5=[t for t in T4 if t not in sw]\n",
        "  T6=[t for t in T5 if len(t)>3]\n",
        "  T7=[unidecode(t) for t in T6 ]\n",
        "  ss = SnowballStemmer('spanish')\n",
        "  T8=[ss.stem(t) for t in T7 ]\n",
        "\n",
        "\n",
        "T8\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nair',\n",
              " 'campeon',\n",
              " 'colombian',\n",
              " 'gan',\n",
              " 'titul',\n",
              " 'carrer',\n",
              " 'hac',\n",
              " 'cuatr',\n",
              " 'anos',\n",
              " 'boyacens',\n",
              " 'nuev',\n",
              " 'campeon',\n",
              " 'oficial',\n",
              " 'carrer',\n",
              " 'descalific',\n",
              " 'campeon',\n",
              " 'oficializ',\n",
              " 'viern',\n",
              " 'nair',\n",
              " 'quintan',\n",
              " 'campeon',\n",
              " 'vuelt',\n",
              " 'asturi',\n",
              " 'edicion',\n",
              " '2017',\n",
              " 'colombian',\n",
              " 'sid',\n",
              " 'segund',\n",
              " 'prueb',\n",
              " 'ganador',\n",
              " 'descalific',\n",
              " 'dopaj',\n",
              " 'espanol',\n",
              " 'raul',\n",
              " 'alarcon',\n",
              " 'nac',\n",
              " 'alic',\n",
              " 'hac',\n",
              " 'anos',\n",
              " 'sid',\n",
              " 'ganador',\n",
              " 'carrer',\n",
              " 'nair',\n",
              " 'termin',\n",
              " 'segund',\n",
              " 'sancion',\n",
              " 'cuatr',\n",
              " 'anos',\n",
              " 'union',\n",
              " 'ciclist',\n",
              " 'internacional',\n",
              " 'metod',\n",
              " 'sustanci',\n",
              " 'prohib',\n",
              " 'suspension',\n",
              " 'provisional',\n",
              " 'octubr',\n",
              " '2019',\n",
              " 'ahor',\n",
              " 'oficial',\n",
              " 'octubr',\n",
              " '2023',\n",
              " 'result',\n",
              " 'consegu',\n",
              " 'moment',\n",
              " 'descont',\n",
              " 'record',\n",
              " 'tras',\n",
              " 'castig',\n",
              " 'ciclist',\n",
              " 'equip',\n",
              " 'port',\n",
              " 'nair',\n",
              " 'logr',\n",
              " 'titul',\n",
              " 'deb',\n",
              " 'segund',\n",
              " 'posicion',\n",
              " 'carrer',\n",
              " 'todavi',\n",
              " 'haci',\n",
              " 'part',\n",
              " 'movist',\n",
              " 'team',\n",
              " 'confirmacion',\n",
              " 'sancion',\n",
              " 'ganador',\n",
              " 'vuelt',\n",
              " 'asturi',\n",
              " '2017',\n",
              " 'raul',\n",
              " 'alarcon',\n",
              " 'otorg',\n",
              " 'form',\n",
              " 'inmediat',\n",
              " 'corredor',\n",
              " 'colombian',\n",
              " 'nair',\n",
              " 'quintan',\n",
              " 'entonc',\n",
              " 'equip',\n",
              " 'movist',\n",
              " 'victori',\n",
              " 'vueltin',\n",
              " 'senal',\n",
              " 'alarcon',\n",
              " 'perderi',\n",
              " 'titul',\n",
              " 'vuelt',\n",
              " 'portugal',\n",
              " 'gan',\n",
              " '2017',\n",
              " '2018',\n",
              " 'segund',\n",
              " 'puest',\n",
              " 'ascendi',\n",
              " 'hispan',\n",
              " 'colombian',\n",
              " 'oscar',\n",
              " 'sevill',\n",
              " 'ultim',\n",
              " 'lustr',\n",
              " 'integr',\n",
              " 'equip',\n",
              " 'pais',\n",
              " 'nair',\n",
              " 'actual',\n",
              " 'hac',\n",
              " 'part',\n",
              " 'equip',\n",
              " 'arke',\n",
              " 'samsik',\n",
              " 'franc',\n",
              " 'compit',\n",
              " 'tirren',\n",
              " 'adriat',\n",
              " 'carrer',\n",
              " 'gan',\n",
              " 'ocasion',\n",
              " '2015',\n",
              " '2017',\n",
              " 'ahor',\n",
              " 'quintan',\n",
              " 'buen',\n",
              " 'desempen',\n",
              " 'tres',\n",
              " 'primer',\n",
              " 'etap',\n",
              " 'posicion',\n",
              " 'segund',\n",
              " 'lid',\n",
              " 'wout',\n",
              " 'aert',\n",
              " 'esper',\n",
              " 'lleg',\n",
              " 'alta',\n",
              " 'montan',\n",
              " 'carrer',\n",
              " 'jorn',\n",
              " 'sab',\n",
              " 'sab',\n",
              " 'disput',\n",
              " 'cuart',\n",
              " 'etap',\n",
              " 'recorr',\n",
              " 'terni',\n",
              " 'prati',\n",
              " 'tiv',\n",
              " 'puert',\n",
              " 'categori',\n",
              " 'especial',\n",
              " 'mayor',\n",
              " 'dificult',\n",
              " 'ascens',\n",
              " 'final',\n",
              " 'medi',\n",
              " 'pic',\n",
              " 'estacion',\n",
              " 'esqui',\n",
              " 'curv',\n",
              " 'cerr',\n",
              " 'mayor',\n",
              " 'pendient',\n",
              " 'primer',\n",
              " 'part',\n",
              " 'pic',\n",
              " 'cruc',\n",
              " 'pietracamel',\n",
              " 'encuentr',\n",
              " 'sprint',\n",
              " 'intermedi',\n",
              " 'ultim',\n",
              " 'tres',\n",
              " 'kilometr',\n",
              " 'primer',\n",
              " 'carrer',\n",
              " 'world',\n",
              " 'tour',\n",
              " 'boyacens',\n",
              " 'afront',\n",
              " 'lleg',\n",
              " 'anim',\n",
              " 'alto',\n",
              " 'mostr',\n",
              " 'gran',\n",
              " 'nivel',\n",
              " 'pes',\n",
              " 'hab',\n",
              " 'sid',\n",
              " 'somet',\n",
              " 'operacion',\n",
              " 'rodill',\n",
              " 'seman',\n",
              " 'pas',\n",
              " 'ocup',\n",
              " 'cuart',\n",
              " 'lug',\n",
              " 'industri',\n",
              " 'artigianat',\n",
              " 'protagon',\n",
              " 'mejor',\n",
              " 'ataqu',\n",
              " 'lleg',\n",
              " 'esprint',\n",
              " 'reduc',\n",
              " 'trat',\n",
              " 'prueb',\n",
              " 'nivel',\n",
              " 'superior',\n",
              " 'esper',\n",
              " 'pod',\n",
              " 'mejor',\n",
              " 'equip',\n",
              " 'much',\n",
              " 'gan',\n",
              " 'hac',\n",
              " 'bien',\n",
              " 'tirren',\n",
              " 'adriat',\n",
              " 'dij',\n",
              " 'boyacens',\n",
              " 'personal',\n",
              " 'carrer',\n",
              " 'encant',\n",
              " 'gan',\n",
              " 'vec',\n",
              " 'cre',\n",
              " 'inclus',\n",
              " 'favorit',\n",
              " 'nuev',\n",
              " 'tirren',\n",
              " 'llen',\n",
              " 'motivacion',\n",
              " 'asegur']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GayoecZnimE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Taller y corpus preparado por Juan Felipe Muñoz Fernández | jfmunozf@unal.edu.co "
      ]
    }
  ]
}